{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36837,"status":"ok","timestamp":1675477528503,"user":{"displayName":"ä¸­ç”°å…‰éš†","userId":"06690068059315868411"},"user_tz":-540},"id":"lBPCEakBFqRg","outputId":"683bd751-fbe0-403c-bcb2-54cc0b0a3d46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1675477528505,"user":{"displayName":"ä¸­ç”°å…‰éš†","userId":"06690068059315868411"},"user_tz":-540},"id":"IOrW-z0yGPGB","outputId":"dccef7ed-d097-4e3c-cb1a-81de0fb32770"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/pml_pl_defect\n"]}],"source":["import os\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/pml_pl_defect')\n","\n","!pwd"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28396,"status":"ok","timestamp":1675477556892,"user":{"displayName":"ä¸­ç”°å…‰éš†","userId":"06690068059315868411"},"user_tz":-540},"id":"PJS1_2lEGYwN","outputId":"4384eb83-783a-4d1f-cf9a-1384b95780fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m109.2/109.2 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.9/178.9 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.1/154.1 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.0/117.0 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q timm wandb pytorch-metric-learning[with-hooks] torchmetrics pytorch-lightning\n","!pip install -q hydra-core --upgrade"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28821,"status":"ok","timestamp":1675477585699,"user":{"displayName":"ä¸­ç”°å…‰éš†","userId":"06690068059315868411"},"user_tz":-540},"id":"WhfZ9ZdV3I0_","outputId":"8bae9bcd-fc61-4632-83b0-30a03c683fa8"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["!wandb login"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WgZUFYRDGyfc","outputId":"da140cc5-eb44-4bb3-879b-289c3e8ecc31","executionInfo":{"status":"ok","timestamp":1675477809219,"user_tz":-540,"elapsed":223531,"user":{"displayName":"ä¸­ç”°å…‰éš†","userId":"06690068059315868411"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33minsilicomab\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Colab Notebooks/pml_pl_defect/wandb/run-20230204_022642-04tvkvsl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbrilliant-firecracker-7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/insilicomab/pml-pl-defect\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/insilicomab/pml-pl-defect/runs/04tvkvsl\u001b[0m\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","  rank_zero_warn(\n","Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base_1k_224_ema.pth\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:467: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n","  rank_zero_deprecation(\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /content/drive/MyDrive/Colab Notebooks/pml_pl_defect/wandb/run-20230204_022642-04tvkvsl/files exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n","Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:1600: PossibleUserWarning: The number of training batches (47) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","  rank_zero_warn(\n","Epoch 0:  43% 47/110 [00:42<00:56,  1.11it/s, v_num=kvsl]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/63 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/63 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  44% 48/110 [00:42<00:54,  1.13it/s, v_num=kvsl]\n","Epoch 0:  45% 49/110 [00:42<00:52,  1.15it/s, v_num=kvsl]\n","Epoch 0:  45% 50/110 [00:42<00:51,  1.17it/s, v_num=kvsl]\n","Epoch 0:  46% 51/110 [00:42<00:49,  1.20it/s, v_num=kvsl]\n","Epoch 0:  47% 52/110 [00:42<00:47,  1.22it/s, v_num=kvsl]\n","Epoch 0:  48% 53/110 [00:42<00:45,  1.24it/s, v_num=kvsl]\n","Epoch 0:  49% 54/110 [00:43<00:44,  1.25it/s, v_num=kvsl]\n","Epoch 0:  50% 55/110 [00:43<00:43,  1.27it/s, v_num=kvsl]\n","Epoch 0:  51% 56/110 [00:43<00:41,  1.29it/s, v_num=kvsl]\n","Epoch 0:  52% 57/110 [00:43<00:40,  1.30it/s, v_num=kvsl]\n","Epoch 0:  53% 58/110 [00:43<00:39,  1.32it/s, v_num=kvsl]\n","Epoch 0:  54% 59/110 [00:44<00:38,  1.34it/s, v_num=kvsl]\n","Epoch 0:  55% 60/110 [00:44<00:36,  1.35it/s, v_num=kvsl]\n","Epoch 0:  55% 61/110 [00:44<00:35,  1.37it/s, v_num=kvsl]\n","Epoch 0:  56% 62/110 [00:44<00:34,  1.38it/s, v_num=kvsl]\n","Epoch 0:  57% 63/110 [00:45<00:33,  1.40it/s, v_num=kvsl]\n","Epoch 0:  58% 64/110 [00:45<00:32,  1.41it/s, v_num=kvsl]\n","Epoch 0:  59% 65/110 [00:45<00:31,  1.43it/s, v_num=kvsl]\n","Epoch 0:  60% 66/110 [00:45<00:30,  1.44it/s, v_num=kvsl]\n","Epoch 0:  61% 67/110 [00:45<00:29,  1.46it/s, v_num=kvsl]\n","Epoch 0:  62% 68/110 [00:46<00:28,  1.47it/s, v_num=kvsl]\n","Epoch 0:  63% 69/110 [00:46<00:27,  1.49it/s, v_num=kvsl]\n","Epoch 0:  64% 70/110 [00:46<00:26,  1.50it/s, v_num=kvsl]\n","Epoch 0:  65% 71/110 [00:46<00:25,  1.52it/s, v_num=kvsl]\n","Epoch 0:  65% 72/110 [00:47<00:24,  1.53it/s, v_num=kvsl]\n","Epoch 0:  66% 73/110 [00:47<00:23,  1.55it/s, v_num=kvsl]\n","Epoch 0:  67% 74/110 [00:47<00:23,  1.56it/s, v_num=kvsl]\n","Epoch 0:  68% 75/110 [00:47<00:22,  1.58it/s, v_num=kvsl]\n","Epoch 0:  69% 76/110 [00:47<00:21,  1.58it/s, v_num=kvsl]\n","Epoch 0:  70% 77/110 [00:48<00:20,  1.60it/s, v_num=kvsl]\n","Epoch 0:  71% 78/110 [00:48<00:19,  1.61it/s, v_num=kvsl]\n","Epoch 0:  72% 79/110 [00:48<00:19,  1.63it/s, v_num=kvsl]\n","Epoch 0:  73% 80/110 [00:48<00:18,  1.64it/s, v_num=kvsl]\n","Epoch 0:  74% 81/110 [00:48<00:17,  1.66it/s, v_num=kvsl]\n","Epoch 0:  75% 82/110 [00:49<00:16,  1.67it/s, v_num=kvsl]\n","Epoch 0:  75% 83/110 [00:49<00:16,  1.68it/s, v_num=kvsl]\n","Epoch 0:  76% 84/110 [00:49<00:15,  1.69it/s, v_num=kvsl]\n","Epoch 0:  77% 85/110 [00:49<00:14,  1.71it/s, v_num=kvsl]\n","Epoch 0:  78% 86/110 [00:50<00:13,  1.72it/s, v_num=kvsl]\n","Epoch 0:  79% 87/110 [00:50<00:13,  1.73it/s, v_num=kvsl]\n","Epoch 0:  80% 88/110 [00:50<00:12,  1.74it/s, v_num=kvsl]\n","Epoch 0:  81% 89/110 [00:50<00:11,  1.76it/s, v_num=kvsl]\n","Epoch 0:  82% 90/110 [00:51<00:11,  1.76it/s, v_num=kvsl]\n","Epoch 0:  83% 91/110 [00:51<00:10,  1.78it/s, v_num=kvsl]\n","Epoch 0:  84% 92/110 [00:51<00:10,  1.79it/s, v_num=kvsl]\n","Epoch 0:  85% 93/110 [00:51<00:09,  1.80it/s, v_num=kvsl]\n","Epoch 0:  85% 94/110 [00:51<00:08,  1.81it/s, v_num=kvsl]\n","Epoch 0:  86% 95/110 [00:52<00:08,  1.83it/s, v_num=kvsl]\n","Epoch 0:  87% 96/110 [00:52<00:07,  1.83it/s, v_num=kvsl]\n","Epoch 0:  88% 97/110 [00:52<00:07,  1.85it/s, v_num=kvsl]\n","Epoch 0:  89% 98/110 [00:52<00:06,  1.86it/s, v_num=kvsl]\n","Epoch 0:  90% 99/110 [00:52<00:05,  1.87it/s, v_num=kvsl]\n","Epoch 0:  91% 100/110 [00:53<00:05,  1.88it/s, v_num=kvsl]\n","Epoch 0:  92% 101/110 [00:53<00:04,  1.89it/s, v_num=kvsl]\n","Epoch 0:  93% 102/110 [00:53<00:04,  1.90it/s, v_num=kvsl]\n","Epoch 0:  94% 103/110 [00:53<00:03,  1.91it/s, v_num=kvsl]\n","Epoch 0:  95% 104/110 [00:54<00:03,  1.92it/s, v_num=kvsl]\n","Epoch 0:  95% 105/110 [00:54<00:02,  1.94it/s, v_num=kvsl]\n","Epoch 0:  96% 106/110 [00:54<00:02,  1.94it/s, v_num=kvsl]\n","Epoch 0:  97% 107/110 [00:54<00:01,  1.96it/s, v_num=kvsl]\n","Epoch 0:  98% 108/110 [00:54<00:01,  1.97it/s, v_num=kvsl]\n","Epoch 0:  99% 109/110 [00:55<00:00,  1.98it/s, v_num=kvsl]\n","Epoch 0: 100% 110/110 [00:55<00:00,  1.99it/s, v_num=kvsl, val_loss=10.80]\n","Epoch 0: 100% 110/110 [00:55<00:00,  1.99it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]tcmalloc: large alloc 1164484608 bytes == 0xa89bc000 @  0x7f5647004680 0x7f5647024da2 0x5f714c 0x64d800 0x527022 0x5c4520 0x5f6eb7 0x7f5604802795 0x7f560495aeeb 0x7f55ddcfcc45 0x7f55ddcf6728 0x7f55ddcfe2c9 0x7f560496e9ca 0x7f5604555f00 0x5f5b39 0x5f6706 0x50ba83 0x570b82 0x569d8a 0x5f60c3 0x56bab6 0x569d8a 0x5f60c3 0x570b82 0x5f5ee6 0x56bab6 0x569d8a 0x50b3a0 0x56cc92 0x569d8a 0x50b3a0\n","Epoch 1:  43% 47/110 [00:18<00:24,  2.53it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Validation: 0it [00:00, ?it/s]\u001b[A\n","Validation:   0% 0/63 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0:   0% 0/63 [00:00<?, ?it/s]\u001b[A\n","Epoch 1:  44% 48/110 [00:18<00:24,  2.55it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  45% 49/110 [00:18<00:23,  2.60it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  45% 50/110 [00:18<00:22,  2.65it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  46% 51/110 [00:18<00:21,  2.70it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  47% 52/110 [00:18<00:21,  2.75it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  48% 53/110 [00:18<00:20,  2.80it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  49% 54/110 [00:18<00:19,  2.84it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  50% 55/110 [00:19<00:19,  2.89it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  51% 56/110 [00:19<00:18,  2.94it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  52% 57/110 [00:19<00:17,  2.99it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  53% 58/110 [00:19<00:17,  3.04it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  54% 59/110 [00:19<00:16,  3.09it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  55% 60/110 [00:19<00:15,  3.14it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  55% 61/110 [00:19<00:15,  3.18it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  56% 62/110 [00:19<00:14,  3.23it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  57% 63/110 [00:19<00:14,  3.28it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  58% 64/110 [00:19<00:13,  3.33it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  59% 65/110 [00:19<00:13,  3.37it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  60% 66/110 [00:19<00:12,  3.42it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  61% 67/110 [00:19<00:12,  3.47it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  62% 68/110 [00:19<00:11,  3.51it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  63% 69/110 [00:19<00:11,  3.56it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  64% 70/110 [00:19<00:11,  3.61it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  65% 71/110 [00:19<00:10,  3.65it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  65% 72/110 [00:19<00:10,  3.70it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  66% 73/110 [00:19<00:09,  3.75it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  67% 74/110 [00:19<00:09,  3.80it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  68% 75/110 [00:19<00:09,  3.84it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  69% 76/110 [00:19<00:08,  3.89it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  70% 77/110 [00:19<00:08,  3.94it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  71% 78/110 [00:19<00:08,  3.98it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  72% 79/110 [00:19<00:07,  4.03it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  73% 80/110 [00:19<00:07,  4.07it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  74% 81/110 [00:19<00:07,  4.12it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  75% 82/110 [00:19<00:06,  4.16it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  75% 83/110 [00:19<00:06,  4.21it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  76% 84/110 [00:19<00:06,  4.25it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  77% 85/110 [00:19<00:05,  4.30it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  78% 86/110 [00:19<00:05,  4.35it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  79% 87/110 [00:19<00:05,  4.39it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  80% 88/110 [00:19<00:04,  4.44it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  81% 89/110 [00:19<00:04,  4.48it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  82% 90/110 [00:19<00:04,  4.53it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  83% 91/110 [00:19<00:04,  4.57it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  84% 92/110 [00:19<00:03,  4.62it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  85% 93/110 [00:19<00:03,  4.66it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  85% 94/110 [00:19<00:03,  4.70it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  86% 95/110 [00:20<00:03,  4.75it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  87% 96/110 [00:20<00:02,  4.79it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  88% 97/110 [00:20<00:02,  4.83it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  89% 98/110 [00:20<00:02,  4.88it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  90% 99/110 [00:20<00:02,  4.92it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  91% 100/110 [00:20<00:02,  4.97it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  92% 101/110 [00:20<00:01,  5.01it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  93% 102/110 [00:20<00:01,  5.05it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  94% 103/110 [00:20<00:01,  5.10it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  95% 104/110 [00:20<00:01,  5.14it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  95% 105/110 [00:20<00:00,  5.18it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  96% 106/110 [00:20<00:00,  5.23it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  97% 107/110 [00:20<00:00,  5.27it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  98% 108/110 [00:20<00:00,  5.31it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1:  99% 109/110 [00:20<00:00,  5.35it/s, v_num=kvsl, val_loss=10.80, train_loss=19.10]\n","Epoch 1: 100% 110/110 [00:20<00:00,  5.39it/s, v_num=kvsl, val_loss=0.896, train_loss=19.10]\n","Epoch 1: 100% 110/110 [00:27<00:00,  3.95it/s, v_num=kvsl, val_loss=0.896, train_loss=4.940]\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:               epoch â–â–â–ˆâ–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss â–ˆâ–\n","\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step â–â–â–ˆâ–ˆ\n","\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss â–ˆâ–\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 1\n","\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 4.93935\n","\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 93\n","\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.89638\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mbrilliant-firecracker-7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/insilicomab/pml-pl-defect/runs/04tvkvsl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 2 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230204_022642-04tvkvsl/logs\u001b[0m\n"]}],"source":["!python train.py"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.3 64-bit ('3.10.3')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.3 (main, Mar 21 2022, 20:30:42) [Clang 13.1.6 (clang-1316.0.21.2)]"},"vscode":{"interpreter":{"hash":"199558dbdb0489c9ed924a71da77983f21ef299bac2d31ff6534c27fc1b6fe26"}}},"nbformat":4,"nbformat_minor":0}